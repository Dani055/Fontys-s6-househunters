<!DOCTYPE html> <html> <body> <p>In order to prove that the application can scale, I ran a local Kubernetes cluster on my machine with Minikube. The deployment on the cluster consists of the following pieces:</p><ul><li><p>Deployments for each type of microservice and frontend</p></li><li><p>Pods of microservices and frontend</p></li><li><p>HorizontalPodAutoscalers</p></li><li><p>StatefulSet of RabbitMQ</p></li><li><p>ClusterIP services to communicate with the pods</p></li><li><p>Ingress controller to manage the incoming requests to the cluster</p></li></ul><p>In order to keep this piece of evidence short, below can be seen a yaml file of only the authentication microservice. The rest of the services are deployed in a similar manner:</p><pre><code>apiVersion: apps/v1 kind: Deployment metadata: name: auth-ms-deployment namespace: househunters spec: replicas: 1 selector: matchLabels: app: auth-ms template: metadata: labels: app: auth-ms spec: containers: - name: auth-ms-container image: ydoykov/s6-househunters-auth-ms:latest imagePullPolicy: Always ports: - containerPort: 9998 resources: limits: cpu: 250m memory: 256Mi requests: cpu: 150m memory: 128Mi --- apiVersion: v1 kind: Service metadata: labels: app: auth-ms name: auth-ms-service namespace: househunters spec: ports: - name: http # Internal cluster port port: 9998 # Port inside of pod targetPort: 9998 selector: # Pods with label app: auth-ms --- apiVersion: autoscaling/v1 kind: HorizontalPodAutoscaler metadata: name: auth-ms-autoscaler namespace: househunters spec: scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: auth-ms-deployment minReplicas: 1 maxReplicas: 10 targetCPUUtilizationPercentage: 50</code></pre><p>The yaml file defines 3 things:</p><ul><li><p>Deployment for authentication microservice with pods</p></li><li><p>Service to load balance requests to the pods</p></li><li><p>HPA used for automatic pod scaling</p></li></ul><p>The selected metric by which to scale a certain deployment is the CPU load. The reason for this is that through testing (as seen a bit below) the REST APIs consume mostly CPU resources and remain relatively light under memory resources. In my scenario, the APIs deal with small requests (there are no requests involving transfers of large files) so memory usage is not a good metric that indicates whether the service is under load. By my expectations, if the microservices were to have more complicated features that dealt with larger requests and computations that they need to process memory would start being a (secondary) metric to scale a service. However, at this scale it is not easily achievable. Even after making attempts to heavily limit the memory dedicated to the service to 64Mb, the pods refused to start and timed out, since express requires about 80Mb. </p><p>The HPA are also really good at eliminating false positives when it comes to scaling. Once a pod is under load, it takes up to 1 minute of sustained load before it gets automatically scaled and up to 5 minutes with lack of load for the service to be scaled down. Kubernetes also considers the average amount of CPU utilized across all pods when deciding whether to scale up or down a specific service.</p><p><img src="https://portfolio.drieam.app/api/public/v1/blobs/545ea1b8-b9c6-4fdf-9e1d-cab50ed05fbf/eyJfcmFpbHMiOnsibWVzc2FnZSI6IkJBaHBBK2xpQXc9PSIsImV4cCI6bnVsbCwicHVyIjoiYmxvYl9pZCJ9fQ==--4d6a69e3b06c47bb22b1cad6e9a0b90eae088de1/image_2024-04-22_113444630.png" data-key="dpe218e6tyifcq1jzn1dzf6qijee"></p><p>The pods are set to request resources of 150m (0.15 CPU cores) and 128Mi of RAM, with a limit of 250m and 256Mi of ram. The autoscaler is configured to deploy more pods when the average load of CPU is more that 50% of 150m. The maximum amount of pods that can run at the same time is 10.<br>To prove that the <code>auth-ms</code> can scale with the HPA, we are going to define the <code>targetCPUUtilizationPercentage</code> as a really small number, for example 10%. On standby, the cluster contains only 1 pod of <code>auth-ms</code> .<br><img src="https://portfolio.drieam.app/api/public/v1/blobs/545ea1b8-b9c6-4fdf-9e1d-cab50ed05fbf/eyJfcmFpbHMiOnsibWVzc2FnZSI6IkJBaHBBMEltQXc9PSIsImV4cCI6bnVsbCwicHVyIjoiYmxvYl9pZCJ9fQ==--4af7fa7ddc11fc15af52334bc219c8732d928838/image_2024-03-31_135315339.png" data-key="yk5q9els5vwdcqxj5782ziftrwge"><br>Using the command <code>kubectl describe hpa auth-ms-autoscaler --namespace=househunters</code> we can get the info logs for the HPA. They indicate that the HPA is running correctly and is ready to scale the deployment under load.<br><img src="https://portfolio.drieam.app/api/public/v1/blobs/545ea1b8-b9c6-4fdf-9e1d-cab50ed05fbf/eyJfcmFpbHMiOnsibWVzc2FnZSI6IkJBaHBBME1tQXc9PSIsImV4cCI6bnVsbCwicHVyIjoiYmxvYl9pZCJ9fQ==--5ebd7e12a7361cecd87ea741de0f9da27a8e4f70/image_2024-03-31_135506114.png" data-key="7ru1po8ffplk3iowqet7k78ngkcr"><br></p><hr><p>In order to generate some load, we can manually send some login requests using Postman. After sending about 30-40 requests we can see in the Minikube dashboard that a utilization of 27m was reached on the first pod and another one was created.<br><img src="https://portfolio.drieam.app/api/public/v1/blobs/545ea1b8-b9c6-4fdf-9e1d-cab50ed05fbf/eyJfcmFpbHMiOnsibWVzc2FnZSI6IkJBaHBBMFVtQXc9PSIsImV4cCI6bnVsbCwicHVyIjoiYmxvYl9pZCJ9fQ==--b1fd9942df97203372696a9e52e9c4b56587e6c4/image_2024-03-31_140022088.png" data-key="dgt0o1c3lqi31irsm6qve607umqe">Finally, we can check the logs of the HPA again to confirm that the pod was created by it.<br><img src="https://portfolio.drieam.app/api/public/v1/blobs/545ea1b8-b9c6-4fdf-9e1d-cab50ed05fbf/eyJfcmFpbHMiOnsibWVzc2FnZSI6IkJBaHBBMFltQXc9PSIsImV4cCI6bnVsbCwicHVyIjoiYmxvYl9pZCJ9fQ==--cdde778611bdf9dc5bab4e5fea4d629cdd2cf290/image_2024-03-31_140139782.png" data-key="jkzdh4ffddhoeynn0k66jqbeknmj"><img src="https://portfolio.drieam.app/api/public/v1/blobs/545ea1b8-b9c6-4fdf-9e1d-cab50ed05fbf/eyJfcmFpbHMiOnsibWVzc2FnZSI6IkJBaHBBMGNtQXc9PSIsImV4cCI6bnVsbCwicHVyIjoiYmxvYl9pZCJ9fQ==--b06b35b8f07fc65254e0a7c2d9d934ed598feb17/image_2024-03-31_140154640.png" data-key="sizz53eiquky06gxyt4uoe9vtevy"></p><hr><p>After continuing to send more requests through Postman we can see that the service load-balances the incoming requests to the pods equally<br><img src="https://portfolio.drieam.app/api/public/v1/blobs/545ea1b8-b9c6-4fdf-9e1d-cab50ed05fbf/eyJfcmFpbHMiOnsibWVzc2FnZSI6IkJBaHBBMGdtQXc9PSIsImV4cCI6bnVsbCwicHVyIjoiYmxvYl9pZCJ9fQ==--db0be042abb2332c099d6beb2c9fde7db1849dfb/image_2024-03-31_140541600.png" data-key="zbe020vug44uvxy8nezvg1w11d16"></p> </body> </html>